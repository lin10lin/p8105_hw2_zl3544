p8105_hw2_zl3544
================
2024-09-30

## R Markdown

``` r
library(tidyverse)
library(readxl)
```

# Problem 1

## Using relative path to access the CSV file and clean the data:

``` r
NYC_transit_df = 
  read_csv("./data/NYC_Transit_Data.csv", na=c("NA","", ".")) %>% 
  janitor::clean_names() %>% 
  select(line:entry, vending, ada) %>% 
 mutate(
    entry = case_match(
      entry,
      "YES" ~ TRUE,
      "NO" ~ FALSE
    )
  )
```

Then see the head of the selected dataframe:

``` r
head(NYC_transit_df)
```

    ## # A tibble: 6 × 19
    ##   line     station_name station_latitude station_longitude route1 route2 route3
    ##   <chr>    <chr>                   <dbl>             <dbl> <chr>  <chr>  <chr> 
    ## 1 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ## 2 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ## 3 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ## 4 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ## 5 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ## 6 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ## # ℹ 12 more variables: route4 <chr>, route5 <chr>, route6 <chr>, route7 <chr>,
    ## #   route8 <dbl>, route9 <dbl>, route10 <dbl>, route11 <dbl>,
    ## #   entrance_type <chr>, entry <lgl>, vending <chr>, ada <lgl>

In the data cleaning process above, I: Use the relative path to read the
csv file, then treat “NA”,““,”.” as missing, then select the variable
that worth analyzing in this problem, and clean up variable names,
finally, I convert the entry variable from character to a logical
variable.

After reading, selecting and cleaning the data, the data set to analyze
has 1868 rows and 19 columns. But the data isn’t tidy, for the reason
that different routes are considered as different variables, these
columns contain value information, they can be transformed. \## Answer
questions in the homework \### Distinct stations Caculate the distinct
stations using distinct function:

``` r
distinct_stations= count(distinct(NYC_transit_df, station_name, line))
```

There are 465 distinct stations.

### Caculate the ADA compliant stations:

``` r
ada_compliant_stations= 
  NYC_transit_df %>% 
  filter(ada==TRUE) %>% 
  distinct(station_name, line) %>% 
  nrow() 
```

There are 84 ADA compliant stations. \### Caculate the proportion of
entrance without vending allow entrance Caculate the number of station
entrances / exits without vending allow entrance:

``` r
no_vending_entry=
  filter(NYC_transit_df, vending=="NO") %>% 
  filter(entry=="TRUE") %>% 
  nrow()
```

Then the proportion of station entrances / exits without vending allow
entrance is: 0.3770492 \### Reformat data and information about A
Reformat data:

``` r
Tidy_NYC_transit_df =
  NYC_transit_df %>% 
  mutate(
    across(route8:route11, as.character)
  ) %>% 
  pivot_longer(
    route1:route11,
    names_to="route_name",
    names_prefix="route",
    values_to="train",
    values_drop_na=TRUE
  ) 
```

Caculate distinct stations serve the A train using R:

``` r
stations_serve_A=
  Tidy_NYC_transit_df %>% 
  filter(train=="A") %>% 
  distinct(line, station_name) %>% 
  nrow() 
```

Distinct stations serve the A train is 60.

Caculate the stations that serve the A train, are ADA compliant:

``` r
stations_serve_A_ada=
  Tidy_NYC_transit_df %>% 
  filter(train=="A" & ada=="TRUE") %>% 
  distinct(line, station_name) %>% 
  nrow()
```

Among the stations that serve the A train, 17 are ADA compliant

# Problem 2

## Read and clean the Mr. Trash Wheel sheet:

``` r
mr_df = read_excel("data/Trash_Wheel_Collection_202409.xlsx", sheet = "Mr. Trash Wheel", range = "A2:N653", na = c("NA", "", ",")) %>% 
  filter(!is.na(Dumpster)) %>% 
  janitor::clean_names() %>% 
  mutate(year = as.character(year)) %>% 
  mutate(trash_wheel = "mr") %>%
  mutate(sports_balls = as.integer(sports_balls), homes_powered = (weight_tons*500/30))
```

## Read and clean the Professor Trash Wheel sheet:

``` r
pro_trash_df = read_excel("data/Trash_Wheel_Collection_202409.xlsx", sheet = "Professor Trash Wheel", range = "A2:M120", na = c("NA", "", ",")) %>% 
  filter(!is.na(Dumpster)) %>% 
  janitor::clean_names() %>% 
  mutate(year = as.character(year)) %>% 
  mutate(homes_powered = (weight_tons*500/30)) %>% 
  mutate(trash_wheel = "pro_trash")
```

## Read and clean the Gwynnda Trash Wheel sheet:

``` r
gwy_df = read_excel("data/Trash_Wheel_Collection_202409.xlsx", sheet = "Gwynnda Trash Wheel", range = "A2:L265", na = c("NA", "", ",")) %>% 
  filter(!is.na(Dumpster)) %>% 
  janitor::clean_names() %>% 
  mutate(year = as.character(year)) %>% 
  mutate(trash_wheel = "gwy") %>%
  mutate(homes_powered = (weight_tons*500/30))
```

## Combine the datasets

``` r
tidy_df=
  bind_rows(mr_df, pro_trash_df, gwy_df) %>% 
  janitor::clean_names() %>% 
  relocate(trash_wheel, .before = dumpster) %>% 
  arrange(month, year, date)
head(tidy_df)
```

    ## # A tibble: 6 × 15
    ##   trash_wheel dumpster month year  date                weight_tons
    ##   <chr>          <dbl> <chr> <chr> <dttm>                    <dbl>
    ## 1 mr                52 April 2015  2015-04-08 00:00:00        2.5 
    ## 2 mr                53 April 2015  2015-04-10 00:00:00        3.41
    ## 3 mr                54 April 2015  2015-04-19 00:00:00        1.83
    ## 4 mr                55 April 2015  2015-04-20 00:00:00        3.84
    ## 5 mr                56 April 2015  2015-04-20 00:00:00        3.22
    ## 6 mr                57 April 2015  2015-04-20 00:00:00        3.03
    ## # ℹ 9 more variables: volume_cubic_yards <dbl>, plastic_bottles <dbl>,
    ## #   polystyrene <dbl>, cigarette_butts <dbl>, glass_bottles <dbl>,
    ## #   plastic_bags <dbl>, wrappers <dbl>, sports_balls <int>, homes_powered <dbl>

## Description

The resulted dataset contains 1032 observations, including the
inforamtion of name of trash wheel, the weight and type of crash
collected by each trashwheel.

``` r
total_weight_pro_trash=
  tidy_df %>% 
  filter(trash_wheel == "pro_trash") %>% 
  pull(weight_tons) %>% 
  sum()
```

The total weight of trash collected by Professor Trash Wheel is 246.74
tons.

``` r
cig_butts_gwy= 
  tidy_df %>% 
  filter(trash_wheel == "gwy") %>% 
  filter(year == 2022) %>% 
  filter(month == "June") %>% 
  pull(cigarette_butts) %>% 
  sum()
```

The total number of cigarette butts collected by Gwynnda in June of 2022
is 1.812^{4}.

# Problem 3

## Import the data :

``` r
bakers_df=
  read_csv("./data/gbb_datasets/bakers.csv", na=c("NA","",".")) %>% 
  janitor::clean_names() %>% 
  separate(baker_name, into = c("baker", "last_name"), sep = " ")
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bakes_df=
  read_csv("./data/gbb_datasets/bakes.csv", na=c("NA","",".")) %>% 
  janitor::clean_names()
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
results_df=
  read_csv("./data/gbb_datasets/results.csv",skip=2 ,na=c("NA","",".")) %>% 
  janitor::clean_names()
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

## Correctness and completeness

Use anti_join function to see how different datasets match, to check the
correctness and completeness of the datasets.

``` r
anti_join(bakes_df, bakers_df, by = "baker", "series")
```

    ## # A tibble: 8 × 5
    ##   series episode baker    signature_bake                            show_stopper
    ##    <dbl>   <dbl> <chr>    <chr>                                     <chr>       
    ## 1      2       1 "\"Jo\"" Chocolate Orange CupcakesOrange and Card… Chocolate a…
    ## 2      2       2 "\"Jo\"" Caramelised Onion, Gruyere and Thyme Qui… Raspberry a…
    ## 3      2       3 "\"Jo\"" Stromboli flavored with Mozzarella, Ham,… Unknown     
    ## 4      2       4 "\"Jo\"" Lavender Biscuits                         Blueberry M…
    ## 5      2       5 "\"Jo\"" Salmon and Asparagus Pie                  Apple and R…
    ## 6      2       6 "\"Jo\"" Rum and Raisin Baked Cheesecake           Limoncello …
    ## 7      2       7 "\"Jo\"" Raspberry & Strawberry Mousse Cake        Pain Aux Ra…
    ## 8      2       8 "\"Jo\"" Raspberry and Blueberry Mille Feuille     Mini Victor…

``` r
anti_join(results_df, bakers_df, by = "baker", "series")
```

    ## # A tibble: 8 × 5
    ##   series episode baker  technical result    
    ##    <dbl>   <dbl> <chr>      <dbl> <chr>     
    ## 1      2       1 Joanne        11 IN        
    ## 2      2       2 Joanne        10 IN        
    ## 3      2       3 Joanne         1 IN        
    ## 4      2       4 Joanne         8 IN        
    ## 5      2       5 Joanne         6 IN        
    ## 6      2       6 Joanne         1 STAR BAKER
    ## 7      2       7 Joanne         3 IN        
    ## 8      2       8 Joanne         1 WINNER

``` r
anti_join(bakers_df, results_df, by = "baker", "series")
```

    ## # A tibble: 1 × 6
    ##   baker last_name series baker_age baker_occupation hometown    
    ##   <chr> <chr>      <dbl>     <dbl> <chr>            <chr>       
    ## 1 Jo    Wheatley       2        41 Housewife        Ongar, Essex

The result show that the name Joanne and Jo doesn’t match, both name are
from the same series, so I suppose that their are the same people, note
as Jo

``` r
results_df = 
  results_df  %>% 
  mutate(baker = ifelse(baker == "Joanne", "Jo", baker))
bakes_df = 
  bakes_df %>% 
  mutate(baker = ifelse(baker == '"Jo"', "Jo", baker))

anti_join(bakes_df, bakers_df, by = "baker", "series")
```

    ## # A tibble: 0 × 5
    ## # ℹ 5 variables: series <dbl>, episode <dbl>, baker <chr>,
    ## #   signature_bake <chr>, show_stopper <chr>

``` r
anti_join(results_df, bakers_df, by = "baker", "series")
```

    ## # A tibble: 0 × 5
    ## # ℹ 5 variables: series <dbl>, episode <dbl>, baker <chr>, technical <dbl>,
    ## #   result <chr>

``` r
anti_join(bakers_df, results_df, by = "baker", "series")
```

    ## # A tibble: 0 × 6
    ## # ℹ 6 variables: baker <chr>, last_name <chr>, series <dbl>, baker_age <dbl>,
    ## #   baker_occupation <chr>, hometown <chr>

Rename Jo to the results_df and bakes_df, the dataset all match with
each other.

## Merge the datasets

Merge the datasets together as one single dataset:

``` r
final_df=
  results_df %>% 
  left_join(bakes_df, by = c("baker", "series", "episode")) %>% 
  left_join(bakers_df, by=c("baker", "series")) %>% 
  relocate(series, episode, .before="baker")
write.csv(final_df, file = "data/gbb_datasets/final.csv")
```

## Description

When seeing the raw data, the baker.csv contains the full name of the
baker, however, the bakes.csv and results.csv only include the first
name of the baker, so I split the full name of the baker in the
baker.csv data. Also, in the results.csv, the first two rows contains no
useful information and lead confuse when handle with this data, so I
skip the first two rows. Then I use anti_join function to see how
different datasets match, then I solve the name that not match. After
that, I merge datasets in a order that can include all the information,
then relocate the variable order to make it more reasonable to read.
Finally, I save the merged dataset to the gbb_dataset file.

The final dataset consists all the information in the bakers.csv,
bakes.csv, results.csv datasets and aviod missing data when merge the
datasets together, and the variable is arranged in a logical way,
readers can quickly find the baker given the series and episodes and the
name of the baker.

## Create table showing winner or star baker

Create a reader-friendly table showing the star baker or winner of each
episode in Seasons 5 through 10.

``` r
star_winner_baker_df=
  final_df %>% 
  filter(series %in% c(5,6,7,8,9,10)) %>% 
  filter(result %in% c("STAR BAKER", "WINNER")) %>% 
  select(series, episode, baker, result) %>% 
  arrange(series, episode, baker)
write.csv(star_winner_baker_df, file = "data/gbb_datasets/star_winner_baker.csv")
```

By viewing the results, we know that at each episode in each season
there is a baker rewarded as “STAR BAKER”, at the end of each season,
there will a “WINNER” of the season. Intuitively, the more a baker
rewarded as “STAR BAKER”, the more likely that baker will be the
“WINNER” of the end of season. We can see from the data that Nadiya from
season 6, Candice ini season 7, Sophie in season 8, Rahul in season 9,
but the winner of season 10 and 5 didn’n follow this pattern.

## Handle with the data in viewers.csv

``` r
viewers_df=
  read_csv("./data/gbb_datasets/viewers.csv", na=c("NA","",".")) %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    cols=series_1:series_10,
    names_to="series",
    names_prefix = "series_",
    values_to = "viewer",
    values_drop_na = TRUE 
  ) %>% 
  relocate(series, .before = episode) %>% 
  arrange(series)
head(viewers_df, 10)
```

    ## # A tibble: 10 × 3
    ##    series episode viewer
    ##    <chr>    <dbl>  <dbl>
    ##  1 1            1   2.24
    ##  2 1            2   3   
    ##  3 1            3   3   
    ##  4 1            4   2.6 
    ##  5 1            5   3.03
    ##  6 1            6   2.75
    ##  7 10           1   9.62
    ##  8 10           2   9.38
    ##  9 10           3   8.94
    ## 10 10           4   8.96

The average viewership in Season 1 is 2.77, the average viewership in
Season 5 is 10.0393.
